{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084ad4d7",
   "metadata": {
    "id": "084ad4d7"
   },
   "source": [
    "<h1>NLP Final Project</h1>\n",
    "\n",
    "Course: **Natural Language Processing and Text Analytics**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tU9v4UDn821z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tU9v4UDn821z",
    "outputId": "d52da5c3-53cb-485e-e2fe-e2579b7bb0e6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ea52f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b3ea52f",
    "outputId": "1d3be9b8-d999-4dc0-8e35-d636292a8738"
   },
   "outputs": [],
   "source": [
    "# Library Import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk # Import stopwords with nltk.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Classification libraries\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Metrics and confusion matrix modules\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095e297",
   "metadata": {
    "id": "c095e297"
   },
   "source": [
    "# 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2869b",
   "metadata": {
    "id": "f4f2869b"
   },
   "source": [
    "## 1.1 Data retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752e9fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "0752e9fe",
    "outputId": "41daf3b0-1e97-4e86-b0a3-600e02481ecc"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Pyhon script used to scrape the Billboard song and artist name\n",
    "# Suggested execution in terminal\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#create dataframe\n",
    "df_wiki = pd.DataFrame(columns = ['year', 'rank', 'song', 'artist'])\n",
    "\n",
    "#specify start date(year)\n",
    "year = 2000\n",
    "\n",
    "#send requests from start date to 2021, as 2022 does not have year end charts yet\n",
    "while year < 2022:\n",
    "    #send GET request to wikipedia\n",
    "    response = requests.get('https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_' + str(year))\n",
    "    \n",
    "    #scrape the website's table\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    table = soup.find('table')\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        #concat data with dataframe if table has more than 2 columns, done to detect the right table on the website\n",
    "        if cols != [] and len(cols) > 2:\n",
    "            data = {'year': str(year), 'rank': str(cols[0]), 'song': str(cols[1]).replace('\"', ''),'artist': str(cols[2])}\n",
    "            df_wiki = pd.concat([df_wiki, pd.DataFrame(data, index=[0])])\n",
    "    year += 1\n",
    "    \n",
    "df_wiki.to_csv('BillboardHot100.csv', index = False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a78ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "5c4a78ce",
    "outputId": "5bcb301d-da72-4c42-9c20-01d2dff7f002"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Python script used to retrieve the lyrics of the songs for the both 'unpopular' and 'popular' dataset\n",
    "# Suggested execution in terminal\n",
    "# In[1]:\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from lyricsgenius import Genius\n",
    "import re \n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "#genius = Genius(\"56jLXWAuduLtVnQs0AAhh67wbabeDc8d-AKgFaxCtLaXSepTnUhEXi2XLlOSfCay\")\n",
    "#artist = genius.search_artist(\"Andy Shauf \", max_songs=3, sort=\"title\")\n",
    "#print(artist.songs)\n",
    "ds = pd.read_csv('unpop_songs.csv')\n",
    "#ds1 = ds[1000:]\n",
    "songs = ds['unpop_songs'].to_list()\n",
    "artists = ds['artist'].to_list()\n",
    "\n",
    "lyrics = []\n",
    "def get_lyrics():\n",
    "    # while len(lyrics) != len(end_df): #1\n",
    "    genius = Genius(\"56jLXWAuduLtVnQs0AAhh67wbabeDc8d-AKgFaxCtLaXSepTnUhEXi2XLlOSfCay\")\n",
    "    genius.timeout = 15\n",
    "    #genius.sleep_time = 40  # 2\n",
    "    # or: Genius(token, timeout=15, sleep_time=40)\n",
    "    for track in ds.values:\n",
    "        retries = 0\n",
    "        while retries < 3:\n",
    "            try:\n",
    "                song = genius.search_song(track[1], track[2])\n",
    "            except Timeout as e:\n",
    "                retries += 1\n",
    "                continue\n",
    "            if song is not None:\n",
    "                lyrics_song = song.lyrics.replace(\"\\n\", \" \") # remove \\n -> backspace\n",
    "                lyrics_song = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", lyrics_song) #remove texts between []\n",
    "                lyrics.append(lyrics_song)\n",
    "            else:\n",
    "                lyrics.append(\"null\")\n",
    "            break\n",
    "    return\n",
    "get_lyrics()\n",
    "ds['lyrics'] = lyrics        \n",
    "ds.to_csv('unpop_songs_all.csv', index = False, header=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab316b8",
   "metadata": {
    "id": "aab316b8"
   },
   "source": [
    "## 1.2 Data merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e907e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "a5e907e0",
    "outputId": "3500e03c-6c0b-498d-eed4-760179afd140"
   },
   "outputs": [],
   "source": [
    "# Load the dataset of songs appearing on the Billboard Hot 100 and setting the class popularity for the prediction task to 1\n",
    "pos = pd.read_csv('/content/drive/MyDrive/NLP_Project/BillboardHot100_with_lyrics_full.csv')\n",
    "pos['popularity'] = 1\n",
    "\n",
    "# Load the dataset of songs from artists appearing on the Billboard Hot 100 but that have not make it into the playlist\n",
    "neg = pd.read_csv('/content/drive/MyDrive/NLP_Project/Final_Unpop_Songs_With_Lyrics.csv')\n",
    "# Setting the class popularity for the prediction task to 0\n",
    "neg['popularity']= 0\n",
    "\n",
    "# Rename col for future correct merge\n",
    "neg['song'] = neg['unpop_song']\n",
    "neg = neg.drop('unpop_song', axis=1)\n",
    "\n",
    "# Remove songs present in the popular dataset\n",
    "#print(neg.song.isin(pos.song))\n",
    "neg = (neg[~neg.song.isin(pos.song)])\n",
    "\n",
    "# Dataset merge\n",
    "ds = pos.append(neg, ignore_index=True)\n",
    "ds.reset_index(drop=True) #reset the index\n",
    "#print(ds.shape)\n",
    "#print(ds['popularity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62c9cd",
   "metadata": {
    "id": "fd62c9cd"
   },
   "source": [
    "## 1.3 Data pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4459a09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4459a09",
    "outputId": "e5db8a66-906b-401c-ef7c-413f50cf2395"
   },
   "outputs": [],
   "source": [
    "# Null values\n",
    "print('number of null values:')\n",
    "print(ds['lyrics'].isna().value_counts()) # Number of null values\n",
    "ds = ds[ds['lyrics'].notna()] # remove null values \n",
    "ds.reset_index(drop=True) # reset the index\n",
    "ds = ds.apply(lambda x: x.astype(str).str.lower()) # convert all ds to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa7341",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dfa7341",
    "outputId": "517576f9-7e3a-4ea9-a564-95a363ae7b9e"
   },
   "outputs": [],
   "source": [
    "#ds.apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii')) # Remove emoji from text\n",
    "#ds['rank'] = ds['rank'].astype(int) # convert back to int\n",
    "#ds = ds.drop(\"Unnamed: 0\", axis=1) # Drop col\n",
    "\n",
    "# remove stopwords, punctuation\n",
    "stop = stopwords.words('english')\n",
    "stop.append('lyrics') # add lyrics to the list\n",
    "stop.append('embed') # add embed to the list\n",
    "\n",
    "ds['song'] = ds['song'].str.replace(r\"'\", \"’\")\n",
    "\n",
    "# remove numbers\n",
    "ds['song'] = ds['song'].str.replace('\\d+', ' ')\n",
    "ds['lyrics'] = ds['lyrics'].str.replace('\\d+', ' ')\n",
    "\n",
    "# remove stop words\n",
    "ds['lyrics_wo_stop'] = ds['lyrics'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#remove punctuation\n",
    "ds['lyrics_wo_stop'] = ds['lyrics_wo_stop'].str.replace(r'[^\\w\\s]+', '') \n",
    " \n",
    "#remove emojis\n",
    "ds['lyrics_wo_stop'] = ds['lyrics_wo_stop'].str.replace(r'[^(a-z|A-Z)]', ' ', regex=True).astype('str')\n",
    "\n",
    "#ds['lyrics_wo_stop'].astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "\n",
    "### Further cleaning\n",
    "\n",
    "# select the \"faulty row\" if the first \"song\" character is not the same as the first char in lyrics\n",
    "ds1 = ds.loc[ds['song'].str[:1] != ds['lyrics'].str[:1]] \n",
    "# select the \"faulty row\" if the first \"artist\" character is not the same as the first char in lyrics\n",
    "ds2 = ds1.loc[ds1['artist'].str[:1] != ds1['lyrics'].str[:1]]\n",
    "index_list = ds2.index.to_list()\n",
    "# index_list\n",
    "ds = ds.drop(index_list)\n",
    "print(\"Dataset size (popular and unpopular data together): \", len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96534520",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96534520",
    "outputId": "ddefd0af-7f7a-4b82-b4f7-82f4c3847eb2"
   },
   "outputs": [],
   "source": [
    "# Class balancing \n",
    "# Sample just 1900 of the unpopular songs to make the dataset more balanced\n",
    "\n",
    "split_neg = ds[ds['popularity'] == \"0\"] # pick pop ds\n",
    "split_pos = ds[ds['popularity'] == \"1\"] # pick unpop ds\n",
    "split_neg = split_neg.sample(frac=1).reset_index(drop=True)\n",
    "split_neg = split_neg.head(1900)\n",
    "ds = split_pos.append(split_neg, ignore_index=True)\n",
    "print(\"Final distribution and size of the dataset to train and test the models: \")\n",
    "print(\"Number of rows:\", ds.shape[0])\n",
    "print(\"Number of features: \", ds.shape[1])\n",
    "print(ds['popularity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23544662",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "23544662",
    "outputId": "6958d0b3-a986-4802-8996-27ade4d9feda"
   },
   "outputs": [],
   "source": [
    "\n",
    "#ds.apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii')) # Remove emoji from text\n",
    "#pop_song_ds = ds.drop(\"Unnamed: 0\", axis=1) # Drop col\n",
    "\n",
    "# Insert bucket labels \n",
    "ds_m = ds[ds['popularity'] == \"1\"] \n",
    "#pd.to_numeric(ds_m['rank'])\n",
    "#ds_m['rank'].to_numeric()\n",
    "#print(ds_m.dtypes)\n",
    "ds['year'] = ds['year'].astype(\"float64\")\n",
    "ds_m['char_count'] = ds_m['lyrics'].str.len() # Add number of characters for each songs \n",
    "\n",
    "'''\n",
    "# Length of songs visualization\n",
    "# Show mean length of popular songs over the years\n",
    "ds_m.sort_values(by=['char_count'], ascending=False)\n",
    "ds_m_grouped = ds_m.groupby(['year']).mean()\n",
    "#ds_m_grouped\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "sns.regplot(x=ds_m_grouped.index, y=ds_m_grouped['char_count'], color=\"#5dc2de\").set_title('Average length of songs across the years')\n",
    "sns.set\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293aad8a",
   "metadata": {
    "id": "293aad8a"
   },
   "outputs": [],
   "source": [
    "cols=['Unnamed: 0']\n",
    "ds_m = ds_m.drop(cols, axis=1) # Drop col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09429408",
   "metadata": {
    "id": "09429408"
   },
   "outputs": [],
   "source": [
    "# Setting the buckets \n",
    "# b_1 (Top tier) = 1-34 \n",
    "# b_2 (Top tier) = 34-67 \n",
    "# b_3 (Top tier) = 68-101 \n",
    "ds_m['rank'] = ds_m['rank'].astype(\"float64\")\n",
    "ds_m['range'] = \"\"\n",
    "ds_m['range'] = ds_m['range'].mask(ds_m['rank'].between(1,34), \"b_1\")\n",
    "ds_m['range'] = ds_m['range'].mask(ds_m['rank'].between(34,67), \"b_2\")\n",
    "ds_m['range'] = ds_m['range'].mask(ds_m['rank'].between(68,101), \"b_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78946704",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78946704",
    "outputId": "12ad8a80-c871-45bf-d78f-2ac2a97e4000"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenization ( i think needed so that lemmatization works smoothly)\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(column1):\n",
    "    column1 = word_tokenize(column1)\n",
    "    return column1\n",
    "\n",
    "ds['lyrics_wo_stop'].dropna(inplace=True)\n",
    "ds['lyrics_wo_stop'] = ds['lyrics_wo_stop'].apply(tokenize)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "ds['lyrics_wo_stop']=ds['lyrics_wo_stop'].apply(lambda x:lemmatizer(x))\n",
    "\n",
    "# Tokenization produces list so changing back to just words\n",
    "def rejoin_words(tokenized_column):\n",
    "    return ( \" \".join(tokenized_column))\n",
    "\n",
    "ds['lyrics_wo_stop'] = ds.apply(lambda x: rejoin_words(x['lyrics_wo_stop']), axis=1)\n",
    "\n",
    "# Checking the distribution for the two classes\n",
    "ds['popularity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99cbce",
   "metadata": {
    "id": "cd99cbce"
   },
   "source": [
    "# 2. Binary Classification: 1st research question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96b4ae",
   "metadata": {
    "id": "3e96b4ae"
   },
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = ds['lyrics_wo_stop']\n",
    "Y = ds['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e71a8",
   "metadata": {
    "id": "7e3e71a8"
   },
   "outputs": [],
   "source": [
    "#divide the ds between train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a5520",
   "metadata": {
    "id": "e83a5520"
   },
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "#creating a count vectorizer. although stop words have already been removed, as an extra safety the parameter is set to remove english stopwords\n",
    "cv = CountVectorizer(decode_error='ignore', \n",
    "                              stop_words='english', \n",
    "                              max_features=10000)   #consider 10000 top max_features ordered by term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d3a74",
   "metadata": {
    "id": "520d3a74"
   },
   "outputs": [],
   "source": [
    "#Tf-Idf transformer\n",
    "tfidf=TfidfTransformer() #norm= 'l2' as default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541920fb",
   "metadata": {
    "id": "541920fb"
   },
   "source": [
    "## 2.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c2ca3",
   "metadata": {
    "id": "829c2ca3"
   },
   "outputs": [],
   "source": [
    "#creating a pipeline for the logistic regression formed by: count vectorizer, tfifd transformer and logistic regression \n",
    "pipe_clf_lr = Pipeline(\n",
    "    [('vect', cv),\n",
    "     ('tfidf', tfidf), \n",
    "     ('clf', LogisticRegression( penalty='l2', solver='lbfgs', \n",
    "                                dual=False, tol=1e-3)), \n",
    "     ])\n",
    "\n",
    "#training the model and obtaining the prediction\n",
    "lr_clf = pipe_clf_lr.fit(X_train,y_train)\n",
    "lr_predictions = pipe_clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abdf7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18abdf7b",
    "outputId": "66551672-41e5-4442-83ba-c6fe8a708428"
   },
   "outputs": [],
   "source": [
    "#binary logistic regression classification report\n",
    "print('binary logistic regression classification report')\n",
    "print(classification_report(y_test,lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f77a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "ec9f77a1",
    "outputId": "4ee0a69a-7992-4465-b5d5-91d663a54af3"
   },
   "outputs": [],
   "source": [
    "#binary logistic regression confusion matrix\n",
    "lr_cm= confusion_matrix(y_test,lr_predictions)\n",
    "lr_ac=accuracy_score(lr_predictions,y_test)\n",
    "plt.title(\"\")\n",
    "sns.heatmap(lr_cm,annot=True,fmt=\"d\" ,cbar=True)\n",
    "print('Linear Regression accuracy:',lr_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1d105",
   "metadata": {
    "id": "5ea1d105"
   },
   "source": [
    "## 2.2 Gaussian NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfc9db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20cfc9db",
    "outputId": "0e4af53c-0038-4532-b586-b8cfd4818daa"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#NB classification pipeline formed by: count vectorizer, tf-idf transformer, Funtion transformer and a Gaussian NB. \n",
    "#The function transformer is required as the dense data are required for the Gaussian NB instead of sparse data\n",
    "pipe_clf_nb = Pipeline(\n",
    "    [('vect', cv),\n",
    "     ('tfidf', tfidf),\n",
    "     ('f', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "     ('clf', GaussianNB()), \n",
    "     ])\n",
    "\n",
    "#parameter grid for the Grid search. The same grid will also be used for the multi class classification\n",
    "param = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "}\n",
    "\n",
    "#Grid search to find the best parameters based on the score of f1-macro. \n",
    "#the evaluation is based on 10 cross-validation steps\n",
    "score = 'f1_macro'\n",
    "clf = GridSearchCV(pipe_clf_nb, param, cv=10, scoring=score)\n",
    "#the best model is used to train the data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#binary Gaussian NB classification report\n",
    "print('binary Gaussian NB classification report')\n",
    "print(classification_report(y_test, clf.predict(X_test), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E2QfMdPNhO_5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "E2QfMdPNhO_5",
    "outputId": "186ea320-5b71-4034-ff4a-63e709cf2e13"
   },
   "outputs": [],
   "source": [
    "#binary Gaussian NB confusion matrix\n",
    "nb_predictions = clf.predict(X_test)\n",
    "nb_cm= confusion_matrix(y_test,nb_predictions)\n",
    "nb_ac=accuracy_score(nb_predictions,y_test)\n",
    "plt.title(\"\")\n",
    "sns.heatmap(nb_cm,annot=True,fmt=\"d\" ,cbar=True)\n",
    "print('Gaussian Naive Bayes accuracy:',nb_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d66e87",
   "metadata": {
    "id": "f4d66e87"
   },
   "source": [
    "## 2.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af676e8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af676e8a",
    "outputId": "51660974-7fa3-492e-b9c7-523166694167"
   },
   "outputs": [],
   "source": [
    "#relevant imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#creating a pipeline for the RF classifier formed by: tfifd transformer and RF classifier \n",
    "\n",
    "pipe_clf_rf = Pipeline(\n",
    "    [('tfidf', tfidf),\n",
    "     ('rf', RandomForestClassifier(n_estimators = 100, random_state = 0)), \n",
    "     ])\n",
    "\n",
    "#parameter grid for the Grid search. The same grid will also be used for the multi class classification\n",
    "grid= {'rf__n_estimators' : list(range(10,101,10)),\n",
    "    'rf__max_features' : list(range(6,32,5)),\n",
    "    'rf__criterion': ['gini', 'entropy']}\n",
    "\n",
    "#Grid search to find the best parameters based on the score of f1-macro. \n",
    "#the evaluation is based on 10 cross-validation steps\n",
    "clf = GridSearchCV(pipe_clf_rf, param_grid = grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "#vectorizing the features\n",
    "X_train_vec = cv.fit_transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "#training the best model\n",
    "best_clf = clf.fit(X_train_vec, y_train)\n",
    "\n",
    "rf_pred=best_clf.predict(X_test_vec)\n",
    "\n",
    "#binary Gaussian NB classification report\n",
    "print('binary Gaussian NB classification report')\n",
    "print(classification_report(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mjhr3fh4hSrE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "mjhr3fh4hSrE",
    "outputId": "2a2253f5-3912-436e-cf59-6b45fac76974"
   },
   "outputs": [],
   "source": [
    "#binary Gaussian NB confusion matrix\n",
    "rf_cm= confusion_matrix(y_test,rf_pred)\n",
    "rf_ac=accuracy_score(rf_pred,y_test)\n",
    "plt.title(\"\")\n",
    "sns.heatmap(rf_cm,annot=True,fmt=\"d\" ,cbar=True)\n",
    "print('Random Forest classifier accuracy:',rf_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47dea4",
   "metadata": {
    "id": "8f47dea4"
   },
   "source": [
    "# 3. Multilabel classification: 2nd research question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a035b",
   "metadata": {
    "id": "2b4a035b"
   },
   "outputs": [],
   "source": [
    "#define features and target\n",
    "X_m = ds_m['lyrics_wo_stop']\n",
    "Y_m = ds_m['range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09c7c4",
   "metadata": {
    "id": "8b09c7c4"
   },
   "outputs": [],
   "source": [
    "#divide the popular song data set between train and test set\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_m, Y_m,shuffle=True,stratify=Y_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eab28e",
   "metadata": {
    "id": "80eab28e"
   },
   "source": [
    "## 3.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebfd6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30ebfd6d",
    "outputId": "80cae820-3967-4258-c6d8-1633b4a25cac"
   },
   "outputs": [],
   "source": [
    "#creating a pipeline for the logistic regression formed by: count vectorizer, tfifd transformer and logistic regression \n",
    "pipe_clf_lr = Pipeline(\n",
    "    [('vect', cv),\n",
    "     ('tfidf', tfidf), \n",
    "     ('clf', LogisticRegression( penalty='l2', solver='lbfgs', \n",
    "                                dual=False, tol=1e-3, multi_class='multinomial')), \n",
    "     ])\n",
    "\n",
    "#training the model and making predictions\n",
    "lr_clf_m = pipe_clf_lr.fit(X_train_m, y_train_m)\n",
    "lr_predictions_m = pipe_clf_lr.predict(X_test_m)\n",
    "\n",
    "#3-class logistic regression classification report\n",
    "print('Logistic regression classification report')\n",
    "print(classification_report(y_test_m,lr_predictions_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TEFFbPPAhWil",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "TEFFbPPAhWil",
    "outputId": "d68b602c-8f09-4b45-dbc2-68c08b6f7222"
   },
   "outputs": [],
   "source": [
    "#3-class logistic regression confusion matrix\n",
    "lr_cm= confusion_matrix(y_test_m,lr_predictions_m)\n",
    "lr_ac=accuracy_score(lr_predictions_m,y_test_m)\n",
    "plt.title(\"\")\n",
    "sns.heatmap(lr_cm,annot=True,fmt=\"d\" ,cbar=True)\n",
    "print('Linear Regression accuracy:',lr_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfd4b0",
   "metadata": {
    "id": "82cfd4b0"
   },
   "source": [
    "## 3.2 Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e724b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d9e724b",
    "outputId": "092047b1-9d88-4bc2-9903-eb416c380586"
   },
   "outputs": [],
   "source": [
    "#NB classification pipeline formed by: count vectorizer, tf-idf transformer and a Multinomial NB. \n",
    "pipe_clf_mnb = Pipeline(\n",
    "    [('vect', cv),\n",
    "     ('tfidf', tfidf),\n",
    "     ('clf', MultinomialNB(alpha=0.6)), #alpha for smooting\n",
    "     ])\n",
    "\n",
    "#mnb_clf_m = pipe_clf_mnb.fit(X_train_m, y_train_m)\n",
    "#mnb_predictions_m = pipe_clf_mnb.predict(X_test_m)\n",
    "\n",
    "#Grid search to find the best parameters based on the score of f1-macro. \n",
    "#the evaluation is based on 10 cross-validation steps\n",
    "score = 'f1_macro'\n",
    "clf = GridSearchCV(pipe_clf_mnb, param, cv=10, scoring=score)\n",
    "clf.fit(X_train_m, y_train_m)\n",
    "\n",
    "#3-class NB classification report\n",
    "print('NB classification report')\n",
    "print(classification_report(y_test_m, clf.predict(X_test_m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BDofJs0Ohe2X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "BDofJs0Ohe2X",
    "outputId": "c3d2e5ae-ddea-470a-947d-fb9d2c31bfa9"
   },
   "outputs": [],
   "source": [
    "#3-class NB confusion matrix\n",
    "nb_predictions = clf.predict(X_test_m)\n",
    "nb_cm= confusion_matrix(y_test_m,nb_predictions)\n",
    "nb_ac=accuracy_score(nb_predictions,y_test_m)\n",
    "plt.title(\"\")\n",
    "sns.heatmap(nb_cm,annot=True,fmt=\"d\" ,cbar=True)\n",
    "print('Multinomial Naive Bayes accuracy:',nb_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4afbf",
   "metadata": {
    "id": "6fc4afbf"
   },
   "source": [
    "## 3.3 Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef76a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cef76a4",
    "outputId": "ef3b322d-b07e-41b9-d84b-5232d54cdbb2"
   },
   "outputs": [],
   "source": [
    "#Here the same pipeline as in the RF binary classification is used\n",
    "X_train_vec_m = cv.fit_transform(X_train_m)\n",
    "X_test_vec_m = cv.transform(X_test_m)\n",
    "\n",
    "\n",
    "#Grid search to find the best parameters based on the score of f1-macro. \n",
    "#the evaluation is based on 10 cross-validation steps\n",
    "clf = GridSearchCV(pipe_clf_rf, param_grid = grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "#training the model and making predictions\n",
    "best_clf_m = clf.fit(X_train_vec_m, y_train_m)\n",
    "\n",
    "rf_pred=best_clf_m.predict(X_test_vec_m)\n",
    "\n",
    "#3-class RF classification report\n",
    "print('Random Forest classification report')\n",
    "print(classification_report(y_test_m, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5pt6_znuhmPP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "5pt6_znuhmPP",
    "outputId": "edb159e1-60ea-4532-a239-ecd299ec88cd"
   },
   "outputs": [],
   "source": [
    "#3-class Random Forest confusion matrix\n",
    "rf_cm= confusion_matrix(y_test_m,rf_pred)\n",
    "rf_ac=accuracy_score(rf_pred,y_test_m)\n",
    "plt.title(\"\")\n",
    "sns.heatmap(rf_cm,annot=True,fmt=\"d\" ,cbar=True)\n",
    "print('Random Forest classifier accuracy:',rf_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97894b8",
   "metadata": {
    "id": "f97894b8"
   },
   "source": [
    "# 4. Topic Modeling: 3rd research question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7da90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36e7da90",
    "outputId": "430842f2-5d3b-44b3-bac9-aee8139ff324"
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import pickle\n",
    "import re \n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "ds['lyrics_wo_stop'] = ds['lyrics_wo_stop'].apply(tokenize)\n",
    "#division for popular and unpopular songs\n",
    "popular=ds['lyrics_wo_stop'][0:1902]\n",
    "unpopular=ds['lyrics_wo_stop'][1902:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3a521",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bed3a521",
    "outputId": "d3d10637-0191-4062-a388-fb68a080358e"
   },
   "outputs": [],
   "source": [
    "#creating dictionary with the number of words appearing in a dataset and filtering tokens\n",
    "def proc(ds):\n",
    "    processed = ds.tolist()\n",
    "    dictionary= gensim.corpora.Dictionary(processed)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    print(len(dictionary.iteritems()))\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed]\n",
    "    print('length of corpus: ', len(bow_corpus))\n",
    "    return dictionary,bow_corpus\n",
    "\n",
    "#obtaining bow corpus and dictionary for popular songs\n",
    "a, b = proc(popular)\n",
    "\n",
    "#obtaining bow corpus and dictionary for unpopular songs\n",
    "c, d = proc(unpopular)\n",
    "\n",
    "\n",
    "#creating a model popular songs\n",
    "lda_model_pop= gensim.models.LdaMulticore(b, num_topics=10, \n",
    "                                       id2word=a, passes=2, \n",
    "                                       workers=2)\n",
    "\n",
    "#creating a model unpopular songs\n",
    "lda_model_unpop= gensim.models.LdaMulticore(d, num_topics=10, \n",
    "                                       id2word=c, passes=2, \n",
    "                                       workers=2)\n",
    "\n",
    "\n",
    "#fucntion to print topics\n",
    "def print_topic(lda_model):\n",
    "    for idx,topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx,topic))\n",
    "\n",
    "#printing popular songs topics\n",
    "print('Popular songs topics:')\n",
    "print_topic(lda_model_pop)\n",
    "\n",
    "#printing popular songs topics\n",
    "print('Unpopular songs topics:')\n",
    "print_topic(lda_model_unpop)\n",
    "\n",
    "#vizualization of the topics\n",
    "def topic_viz(lda_model, bow_corpus, dictionary):\n",
    "    vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=bow_corpus, dictionary=dictionary)\n",
    "    pyLDAvis.enable_notebook()\n",
    "    return pyLDAvis.display(vis)\n",
    "\n",
    "#popular songs vizualization\n",
    "pop_viz=topic_viz(lda_model_pop, b, a)\n",
    "pop_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nGlAuz42fYQM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "nGlAuz42fYQM",
    "outputId": "960416d2-6cb4-41ef-b9b3-a9b89f7c66e6"
   },
   "outputs": [],
   "source": [
    "#unpopular songs vizualization\n",
    "unpop_viz=topic_viz(lda_model_unpop, d, c)\n",
    "unpop_viz"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FINAL_VERSION_NLP_PROJECT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
